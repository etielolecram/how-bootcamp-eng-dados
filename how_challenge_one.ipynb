{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KccqfD8ujL3K"
      },
      "source": [
        "How Bootcamp Engenharia de Dados - Desafio 1\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enV-FoDwpwG_"
      },
      "outputs": [],
      "source": [
        "# Para obter os datasets via kaggle-cli é preciso ter o token da conta do Kaggle configurado. \n",
        "# Para isso, basta obter o token e salvar o arquivo no seu Google Drive.\n",
        "# No meu caso, eu salvei em /content/drive/MyDrive/Colab-Notebooks/how\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZmoGRnun5OI"
      },
      "outputs": [],
      "source": [
        "# Definir uma ENV_VAR que aponte para diretório onde se encontra o token do Kaggle\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Colab-Notebooks/how\"\n",
        "\n",
        "# Instalar o kaggle-cli\n",
        "!pip install --upgrade kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JmcO4uWZ3OLO"
      },
      "outputs": [],
      "source": [
        "# instalar outras dependências\n",
        "!apt-get update -qq\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!wget -q https://jdbc.postgresql.org/download/postgresql-42.6.0.jar\n",
        "!mv postgresql-42.6.0.jar /content/spark-3.4.0-bin-hadoop3/jars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kqdft9fpGlH0"
      },
      "outputs": [],
      "source": [
        "# Ambiente spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yayRGdn_GlH1"
      },
      "outputs": [],
      "source": [
        "# Iniciar a sessão spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Bootcamp Engenharia de Dados - How\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "PizPNqbmCuSM",
        "outputId": "ea64a9b2-11f4-446e-dea4-bbf02d51a3f0"
      },
      "outputs": [],
      "source": [
        "# Mostra a sessão spark criada\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "HGoJCNeb-g9-"
      },
      "outputs": [],
      "source": [
        "# Outras variáveis necessárias\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qG3InBVTyJXW"
      },
      "outputs": [],
      "source": [
        "# O banco de dados PostgreSQL usado no exercício pode ser qualquer serviço SAS gratuito (RDS, por exemplo)\n",
        "# Aqui eu usei o serviço elephantsql.com (que até usa a infra da AWS)\n",
        "# Basta criar uma conta e uma instância e obter a URL, login e senha \n",
        "# Seria um procedimento parecido para o RDS com o adicional de ter de criar as regras de Security Group para permitir acesso externo\n",
        "# Criar um arquivo json com a seguinte estrutura\n",
        "# {\n",
        "#  \"DB_URL\": \"valor_url\",\n",
        "#  \"DB_USER\": \"valor_user\",\n",
        "#  \"DB_PWD\": \"valor_pwd\"\n",
        "# }\n",
        "env = spark.read.json('/content/drive/MyDrive/Colab-Notebooks/how/.env', multiLine=True)\n",
        "os.environ[\"DB_URL\"] = env.select('DB_URL').first()[0]\n",
        "os.environ[\"DB_USER\"] = env.select('DB_USER').first()[0]\n",
        "os.environ[\"DB_PWD\"] = env.select('DB_PWD').first()[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NCosmuEwoV-X"
      },
      "source": [
        "# Processando dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GsFaSELATik",
        "outputId": "3362afcd-afa9-4cfe-c0ad-bfd2b365ca5a"
      },
      "outputs": [],
      "source": [
        "# Criar um diretório para os datasets\n",
        "!mkdir datasets\n",
        "\n",
        "# baixar datasets do desafio e mover para o diretório datasets\n",
        "!kaggle datasets download -d chickooo/top-tech-startups-hiring-2023\n",
        "!mv top-tech-startups-hiring-2023.zip datasets\n",
        "\n",
        "!kaggle datasets download -d loganlauton/nba-players-and-team-data\n",
        "!mv nba-players-and-team-data.zip datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "whfpwm-i06av"
      },
      "outputs": [],
      "source": [
        "# Funções utilitárias\n",
        "from pyspark.sql.functions import current_timestamp, col, to_json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CMknJG3r2Fa-"
      },
      "source": [
        "Top Tech Startups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1rDKf8L-GlH6"
      },
      "outputs": [],
      "source": [
        "# Descompactar arquivos\n",
        "! unzip -q -d '/content/datasets' '/content/datasets/top-tech-startups-hiring-2023.zip'\n",
        "! rm -rf '/content/datasets/images' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AnS7LIvJ0-5",
        "outputId": "905c2252-70b3-44c6-ade0-e9bf9016ac56"
      },
      "outputs": [],
      "source": [
        "# Criando o dataframe\n",
        "path = '/content/datasets/json_data.json'\n",
        "df = spark.read.json(path, multiLine=True)\n",
        "df.printSchema()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8L9PuNwsuAOc",
        "outputId": "d01a6d81-0606-4fbd-d7af-bd44f951ea0e"
      },
      "outputs": [],
      "source": [
        "# Transformando a struct jobs em um json\n",
        "df = df.withColumn(\"jobs\",to_json(col(\"jobs\")))\n",
        "\n",
        "# Criando a coluna created_at\n",
        "df = df.withColumn(\"created_at\", current_timestamp())\n",
        "\n",
        "# Salvando os dados no banco\n",
        "df.write.jdbc(\n",
        "    url=os.getenv(\"DB_URL\"), \n",
        "    table=\"top_tech_startups\", \n",
        "    properties={\n",
        "        \"user\": os.getenv(\"DB_USER\"), \n",
        "        \"password\": os.getenv(\"DB_PWD\")})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-gofd1Ipyn8D"
      },
      "source": [
        "## **NBA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sELxkQvTyxPh"
      },
      "outputs": [],
      "source": [
        "# Descompactar arquivos\n",
        "! unzip -q -d '/content/datasets' '/content/datasets/nba-players-and-team-data.zip'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wwVVWwyS1tIn"
      },
      "source": [
        "Payroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vz9cnoyzCMW",
        "outputId": "a249b925-fac2-49dc-ab85-335e621f733c"
      },
      "outputs": [],
      "source": [
        "# Criando o dataframe\n",
        "path = '/content/datasets/NBA Payroll(1990-2023).csv'\n",
        "df = spark.read.option(\"header\",True).csv(path)\n",
        "df.printSchema()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "eHMcORIf0xv9"
      },
      "outputs": [],
      "source": [
        "# Criando uma nova coluna com o valor de _c0\n",
        "df = df.withColumn(\"id\",col(\"_c0\"))\n",
        "\n",
        "# Removendo _c0\n",
        "df = df.drop('_c0')\n",
        "\n",
        "# Criando a coluna created_at\n",
        "df = df.withColumn(\"created_at\", current_timestamp())\n",
        "\n",
        "# Salvando os dados no banco\n",
        "df.write.jdbc(\n",
        "    url=os.getenv(\"DB_URL\"), \n",
        "    table=\"nba_payroll\", \n",
        "    properties={\n",
        "        \"user\": os.getenv(\"DB_USER\"), \n",
        "        \"password\": os.getenv(\"DB_PWD\")})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2DfUydF2M1N"
      },
      "source": [
        "Salaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYF3kCAi4R3S",
        "outputId": "b328431b-00cb-4f6c-fe89-1baaa14f462a"
      },
      "outputs": [],
      "source": [
        "# Criando o dataframe\n",
        "path = '/content/datasets/NBA Salaries(1990-2023).csv'\n",
        "df = spark.read.option(\"header\",True).csv(path)\n",
        "df.printSchema()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "o6sT0bi65zjb"
      },
      "outputs": [],
      "source": [
        "# Criando uma nova coluna com o valor de _c0\n",
        "df = df.withColumn(\"id\",col(\"_c0\"))\n",
        "\n",
        "# Removendo _c0\n",
        "df = df.drop('_c0')\n",
        "\n",
        "# Criando a coluna created_at\n",
        "df = df.withColumn(\"created_at\", current_timestamp())\n",
        "\n",
        "# Salvando os dados no banco\n",
        "df.write.jdbc(\n",
        "    url=os.getenv(\"DB_URL\"), \n",
        "    table=\"nba_salaries\", \n",
        "    properties={\n",
        "        \"user\": os.getenv(\"DB_USER\"), \n",
        "        \"password\": os.getenv(\"DB_PWD\")})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CmveHrFAEDEp"
      },
      "source": [
        "Player Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BclllsIJEF4t"
      },
      "outputs": [],
      "source": [
        "# Criando o dataframe\n",
        "path = '/content/datasets/NBA Player Stats(1950 - 2022).csv'\n",
        "df = spark.read.option(\"header\",True).csv(path)\n",
        "df.printSchema()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "aELd3rf6EVvv"
      },
      "outputs": [],
      "source": [
        "# Criando uma nova coluna com o valor de _c0\n",
        "df = df.withColumn(\"id\",col(\"_c0\"))\n",
        "\n",
        "# Removendo as colubas _c0 e Unnamed\n",
        "df = df.drop('_c0')\n",
        "df = df.drop('Unnamed')\n",
        "\n",
        "# Criando a coluna created_at\n",
        "df = df.withColumn(\"created_at\", current_timestamp())\n",
        "\n",
        "# Salvando os dados no banco\n",
        "df.write.jdbc(\n",
        "    url=os.getenv(\"DB_URL\"), \n",
        "    table=\"nba_player_stats\", \n",
        "    properties={\n",
        "        \"user\": os.getenv(\"DB_USER\"), \n",
        "        \"password\": os.getenv(\"DB_PWD\")})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "INYFxjgZE2cT"
      },
      "source": [
        "Player Box Score Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvQBXFQHE-wl",
        "outputId": "a984b66b-78c4-4216-c489-8f92be0698eb"
      },
      "outputs": [],
      "source": [
        "# Criando o dataframe\n",
        "path = '/content/datasets/NBA Player Box Score Stats(1950 - 2022).csv'\n",
        "df = spark.read.option(\"header\",True).csv(path)\n",
        "df.printSchema()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "RXCEot7KFQ1o"
      },
      "outputs": [],
      "source": [
        "# Criando uma nova coluna com o valor de _c0\n",
        "df50k = df.limit(50000)\n",
        "df50k = df50k.withColumn(\"id\",col(\"_c0\"))\n",
        "\n",
        "# Removendo as colubas _c0\n",
        "df50k = df50k.drop('_c0')\n",
        "\n",
        "# Criando a coluna created_at\n",
        "df50k = df50k.withColumn(\"created_at\", current_timestamp())\n",
        "\n",
        "# Salvando os dados no banco\n",
        "df50k.write.jdbc(\n",
        "    url=os.getenv(\"DB_URL\"), \n",
        "    table=\"nba_player_box_score_stats\", \n",
        "    properties={\n",
        "        \"user\": os.getenv(\"DB_USER\"), \n",
        "        \"password\": os.getenv(\"DB_PWD\")})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
